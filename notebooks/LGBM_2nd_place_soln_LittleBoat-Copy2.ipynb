{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It definitely came as a surprise to me that there was this huge shake up in the end. \n",
    "I personally thought that the shake up should be relatively small because my stacking \n",
    "model performance on public LB and local cv was always aligned although for single model \n",
    "they could vary quite a bit. We did trust mostly on local CV and we tried very hard on nonlinear \n",
    "stacking (without success) so we relied on weighted average cv score and only submitted when there was an improvement on local cv.\n",
    "\n",
    "# I cannot wait to see what @Michael Jahrer did but I would like to share some of our approaches (actually, just the NN part, and leave the rest to my teammates) before Michael publishes his and then nobody cares what we did :(\n",
    "\n",
    "# So my best NN had a 5 fold local CV 0.294, and public LB 0.284 and private 0.290. And here is roughly how you can achieve that:\n",
    "\n",
    "# 1) important features' interactions (e.g. ps_car_13, ps_ind_03, ps_reg_03, ...)\n",
    "\n",
    "# 2) count of categorical features\n",
    "\n",
    "# 3) xgboost predictions: divide feature sets into three groups (car, ind, reg) and then use two group as features and the other group as target, train a xgboost model on it, and use prediction as features\n",
    "\n",
    "# 4) feature aggregation: pick two features (e.g. ps_car_13, ps_ind_03), and then use one as group variable, the other as value variable, do mean, std, max, min, median. Still top important features are picked only\n",
    "\n",
    "# 5) do embedding layers on all categorical features (I used embedding dimension 4 with dropout 0.25)\n",
    "\n",
    "# 6) model is 2 layer with relu activation and big dropout (512 with dropout 0.8, and 64 with 0.8)\n",
    "\n",
    "# 7) a few categorical count features @qianqian created\n",
    "\n",
    "# That is about it! Hope it helps :)\n",
    "\n",
    "# Update: my best lightgbm code is here\n",
    "\n",
    "# https://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution\n",
    "\n",
    "# It scored 0.29124 on private LB.\n",
    "\n",
    "# best nn code is here https://www.kaggle.com/xiaozhouwang/2nd-place-solution-nn-model\n",
    "\n",
    "# It scored 0.29008 on private LB.\n",
    "\n",
    "# And their model results can be found in the comments (Seems I cannot upload files here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part of 2nd place solution: lightgbm model with private score 0.29124 and public lb score 0.28555\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def Gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    # sort rows on prediction column\n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "cv_only = True\n",
    "save_cv = True\n",
    "full_train = False\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True\n",
    "\n",
    "path = \"../data/\"\n",
    "\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "test_id = test['id']\n",
    "\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "y = train['target'].values\n",
    "drop_feature = [\n",
    "    'id',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "X = train.drop(drop_feature,axis=1)\n",
    "feature_names = X.columns.tolist()\n",
    "cat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\n",
    "num_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]\n",
    "\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "num_features.append('missing')\n",
    "\n",
    "for c in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c])\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train[cat_features])\n",
    "X_cat = enc.transform(train[cat_features])\n",
    "X_t_cat = enc.transform(test[cat_features])\n",
    "\n",
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count=0\n",
    "for c in ind_features:\n",
    "    if count==0:\n",
    "        train['new_ind'] = train[c].astype(str)+'_'\n",
    "        test['new_ind'] = test[c].astype(str)+'_'\n",
    "        count+=1\n",
    "    else:\n",
    "        train['new_ind'] += train[c].astype(str)+'_'\n",
    "        test['new_ind'] += test[c].astype(str)+'_'\n",
    "\n",
    "cat_count_features = []\n",
    "for c in cat_features+['new_ind']:\n",
    "    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n",
    "    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n",
    "    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n",
    "    cat_count_features.append('%s_count'%c)\n",
    "\n",
    "train_list = [train[num_features+cat_count_features].values,X_cat,]\n",
    "test_list = [test[num_features+cat_count_features].values,X_t_cat,]\n",
    "\n",
    "X = ssp.hstack(train_list).tocsr()\n",
    "X_test = ssp.hstack(test_list).tocsr()\n",
    "\n",
    "\n",
    "# learning_rate = 0.1\n",
    "# num_leaves = 15\n",
    "# min_data_in_leaf = 2000\n",
    "# feature_fraction = 0.6\n",
    "# num_boost_round = 10000\n",
    "# params = {\"objective\": \"binary\",\n",
    "#           \"boosting_type\": \"gbdt\",\n",
    "#           \"learning_rate\": learning_rate,\n",
    "#           \"num_leaves\": num_leaves,\n",
    "#            \"max_bin\": 256,\n",
    "#           \"feature_fraction\": feature_fraction,\n",
    "#           \"verbosity\": 0,\n",
    "#           \"drop_rate\": 0.1,\n",
    "#           \"is_unbalance\": False,\n",
    "#           \"max_drop\": 50,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "#           \"subsample\": 0.9\n",
    "#           }\n",
    "\n",
    "# x_score = []\n",
    "# final_cv_train = np.zeros(len(train_label))\n",
    "# final_cv_pred = np.zeros(len(test_id))\n",
    "# for s in range(16):\n",
    "#     cv_train = np.zeros(len(train_label))\n",
    "#     cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "#     params['seed'] = s\n",
    "\n",
    "#     if cv_only:\n",
    "#         kf = kfold.split(X, train_label)\n",
    "\n",
    "#         best_trees = []\n",
    "#         fold_scores = []\n",
    "\n",
    "#         for i, (train_fold, validate) in enumerate(kf):\n",
    "#             X_train, X_validate, label_train, label_validate = \\\n",
    "#                 X[train_fold, :], X[validate, :], train_label[train_fold], train_label[validate]\n",
    "#             dtrain = lgbm.Dataset(X_train, label_train)\n",
    "#             dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "#             bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, verbose_eval=100,\n",
    "#                             early_stopping_rounds=100)\n",
    "#             best_trees.append(bst.best_iteration)\n",
    "#             cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "#             cv_train[validate] += bst.predict(X_validate)\n",
    "\n",
    "#             score = Gini(label_validate, cv_train[validate])\n",
    "#             print(score)\n",
    "#             fold_scores.append(score)\n",
    "\n",
    "#         cv_pred /= NFOLDS\n",
    "#         final_cv_train += cv_train\n",
    "#         final_cv_pred += cv_pred\n",
    "\n",
    "#         print(\"cv score:\")\n",
    "#         print(Gini(train_label, cv_train))\n",
    "#         print(\"current score:\", Gini(train_label, final_cv_train / (s + 1.)), s+1)\n",
    "#         print(fold_scores)\n",
    "#         print(best_trees, np.mean(best_trees))\n",
    "\n",
    "#         x_score.append(Gini(train_label, cv_train))\n",
    "\n",
    "# print(x_score)\n",
    "# pd.DataFrame({'id': test_id, 'target': final_cv_pred / 16.}).to_csv('../cache/lgbm3_pred_avg.csv', index=False)\n",
    "# pd.DataFrame({'id': train_id, 'target': final_cv_train / 16.}).to_csv('../cache/lgbm3_cv_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Original author CPMP : https://www.kaggle.com/cpmpml\n",
    "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrn_df):\n",
    "    labels = dtrn_df.get_label()\n",
    "    gini_score = eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<595212x223 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25058414 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "import time \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : 0.274467 @ 200 / best score is 0.274507 @ 199\n",
      "Fold  2 : 0.276190 @ 200 / best score is 0.276228 @ 197\n",
      "Fold  3 : 0.304708 @ 200 / best score is 0.304749 @ 199\n",
      "Fold  4 : 0.280130 @ 200 / best score is 0.280372 @ 197\n",
      "Fold  5 : 0.276845 @ 200 / best score is 0.276845 @ 199\n",
      "1.0\n",
      "Full OOF score : 0.282400\n",
      "Best mean score : 0.282507 + 0.011277 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0175184751861 0.581780485809\n",
      "Fold  1 : 0.274184 @ 200 / best score is 0.274232 @ 199\n",
      "Fold  2 : 0.277669 @ 200 / best score is 0.277800 @ 196\n",
      "Fold  3 : 0.303862 @ 200 / best score is 0.303862 @ 196\n",
      "Fold  4 : 0.280707 @ 200 / best score is 0.280799 @ 189\n",
      "Fold  5 : 0.278447 @ 200 / best score is 0.278510 @ 199\n",
      "1.10526315789\n",
      "Full OOF score : 0.282897\n",
      "Best mean score : 0.282919 + 0.010668 @ 196\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0192520827986 0.607652544975\n",
      "Fold  1 : 0.276843 @ 200 / best score is 0.277000 @ 199\n",
      "Fold  2 : 0.278153 @ 200 / best score is 0.278153 @ 197\n",
      "Fold  3 : 0.304731 @ 200 / best score is 0.304777 @ 199\n",
      "Fold  4 : 0.280060 @ 200 / best score is 0.280163 @ 193\n",
      "Fold  5 : 0.277551 @ 200 / best score is 0.277594 @ 199\n",
      "1.21052631579\n",
      "Full OOF score : 0.283389\n",
      "Best mean score : 0.283473 + 0.010700 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0201001639944 0.63393958658\n",
      "Fold  1 : 0.273926 @ 200 / best score is 0.274090 @ 198\n",
      "Fold  2 : 0.276857 @ 200 / best score is 0.276858 @ 197\n",
      "Fold  3 : 0.302955 @ 200 / best score is 0.303023 @ 199\n",
      "Fold  4 : 0.280131 @ 200 / best score is 0.280200 @ 193\n",
      "Fold  5 : 0.277923 @ 200 / best score is 0.277984 @ 199\n",
      "1.31578947368\n",
      "Full OOF score : 0.282289\n",
      "Best mean score : 0.282390 + 0.010497 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0213223602623 0.677880957723\n",
      "Fold  1 : 0.273995 @ 200 / best score is 0.274051 @ 199\n",
      "Fold  2 : 0.279217 @ 200 / best score is 0.279217 @ 197\n",
      "Fold  3 : 0.305004 @ 200 / best score is 0.305043 @ 192\n",
      "Fold  4 : 0.281039 @ 200 / best score is 0.281265 @ 198\n",
      "Fold  5 : 0.278246 @ 200 / best score is 0.278298 @ 199\n",
      "1.42105263158\n",
      "Full OOF score : 0.283416\n",
      "Best mean score : 0.283546 + 0.010963 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0230591609143 0.678025685251\n",
      "Fold  1 : 0.274306 @ 200 / best score is 0.274319 @ 199\n",
      "Fold  2 : 0.277999 @ 200 / best score is 0.277999 @ 197\n",
      "Fold  3 : 0.303988 @ 200 / best score is 0.304072 @ 192\n",
      "Fold  4 : 0.282210 @ 200 / best score is 0.282219 @ 198\n",
      "Fold  5 : 0.279309 @ 200 / best score is 0.279337 @ 199\n",
      "1.52631578947\n",
      "Full OOF score : 0.283455\n",
      "Best mean score : 0.283558 + 0.010519 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0244532707147 0.691477946937\n",
      "Fold  1 : 0.274876 @ 200 / best score is 0.275111 @ 198\n",
      "Fold  2 : 0.278020 @ 200 / best score is 0.278042 @ 198\n",
      "Fold  3 : 0.304617 @ 200 / best score is 0.304709 @ 192\n",
      "Fold  4 : 0.280857 @ 200 / best score is 0.280866 @ 190\n",
      "Fold  5 : 0.279885 @ 200 / best score is 0.279966 @ 199\n",
      "1.63157894737\n",
      "Full OOF score : 0.283577\n",
      "Best mean score : 0.283670 + 0.010657 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0256053060293 0.699694544077\n",
      "Fold  1 : 0.276327 @ 200 / best score is 0.276328 @ 199\n",
      "Fold  2 : 0.277924 @ 200 / best score is 0.277980 @ 198\n",
      "Fold  3 : 0.304582 @ 200 / best score is 0.304592 @ 196\n",
      "Fold  4 : 0.282394 @ 200 / best score is 0.282662 @ 198\n",
      "Fold  5 : 0.280070 @ 200 / best score is 0.280122 @ 198\n",
      "1.73684210526\n",
      "Full OOF score : 0.284174\n",
      "Best mean score : 0.284325 + 0.010326 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0265520694666 0.727219238877\n",
      "Fold  1 : 0.276004 @ 200 / best score is 0.276147 @ 199\n",
      "Fold  2 : 0.278841 @ 200 / best score is 0.278887 @ 198\n",
      "Fold  3 : 0.304157 @ 200 / best score is 0.304160 @ 199\n",
      "Fold  4 : 0.281386 @ 200 / best score is 0.281481 @ 186\n",
      "Fold  5 : 0.280399 @ 200 / best score is 0.280439 @ 199\n",
      "1.84210526316\n",
      "Full OOF score : 0.284090\n",
      "Best mean score : 0.284153 + 0.010151 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0279630701989 0.741618216038\n",
      "Fold  1 : 0.274820 @ 200 / best score is 0.274876 @ 199\n",
      "Fold  2 : 0.276988 @ 200 / best score is 0.277065 @ 197\n",
      "Fold  3 : 0.303619 @ 200 / best score is 0.303683 @ 192\n",
      "Fold  4 : 0.281726 @ 200 / best score is 0.281835 @ 198\n",
      "Fold  5 : 0.281108 @ 200 / best score is 0.281134 @ 199\n",
      "1.94736842105\n",
      "Full OOF score : 0.283569\n",
      "Best mean score : 0.283619 + 0.010192 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0288042183965 0.737910240889\n",
      "Fold  1 : 0.274915 @ 200 / best score is 0.275094 @ 198\n",
      "Fold  2 : 0.278602 @ 200 / best score is 0.278642 @ 198\n",
      "Fold  3 : 0.303687 @ 200 / best score is 0.303737 @ 192\n",
      "Fold  4 : 0.281017 @ 200 / best score is 0.281285 @ 198\n",
      "Fold  5 : 0.280007 @ 200 / best score is 0.280049 @ 199\n",
      "2.05263157895\n",
      "Full OOF score : 0.283557\n",
      "Best mean score : 0.283713 + 0.010156 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0305259064771 0.74106810987\n",
      "Fold  1 : 0.274797 @ 200 / best score is 0.274924 @ 199\n",
      "Fold  2 : 0.276271 @ 200 / best score is 0.276329 @ 198\n",
      "Fold  3 : 0.304655 @ 200 / best score is 0.304750 @ 198\n",
      "Fold  4 : 0.280682 @ 200 / best score is 0.280837 @ 198\n",
      "Fold  5 : 0.280866 @ 200 / best score is 0.280902 @ 198\n",
      "2.15789473684\n",
      "Full OOF score : 0.283381\n",
      "Best mean score : 0.283523 + 0.010886 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.031562551856 0.753482088447\n",
      "Fold  1 : 0.276858 @ 200 / best score is 0.276979 @ 199\n",
      "Fold  2 : 0.278112 @ 200 / best score is 0.278189 @ 198\n",
      "Fold  3 : 0.304999 @ 200 / best score is 0.305002 @ 199\n",
      "Fold  4 : 0.281968 @ 200 / best score is 0.281981 @ 199\n",
      "Fold  5 : 0.280715 @ 200 / best score is 0.280742 @ 195\n",
      "2.26315789474\n",
      "Full OOF score : 0.284455\n",
      "Best mean score : 0.284557 + 0.010378 @ 199\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0314585291781 0.759206831455\n",
      "Fold  1 : 0.276184 @ 200 / best score is 0.276366 @ 198\n",
      "Fold  2 : 0.277393 @ 200 / best score is 0.277416 @ 199\n",
      "Fold  3 : 0.304491 @ 200 / best score is 0.304571 @ 188\n",
      "Fold  4 : 0.281848 @ 200 / best score is 0.281910 @ 196\n",
      "Fold  5 : 0.279325 @ 200 / best score is 0.279369 @ 199\n",
      "2.36842105263\n",
      "Full OOF score : 0.283766\n",
      "Best mean score : 0.283865 + 0.010478 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0347527330741 0.771692112088\n",
      "Fold  1 : 0.275202 @ 200 / best score is 0.275307 @ 199\n",
      "Fold  2 : 0.277455 @ 200 / best score is 0.277530 @ 168\n",
      "Fold  3 : 0.303315 @ 200 / best score is 0.303398 @ 192\n",
      "Fold  4 : 0.280827 @ 200 / best score is 0.280884 @ 189\n",
      "Fold  5 : 0.279629 @ 200 / best score is 0.279677 @ 196\n",
      "2.47368421053\n",
      "Full OOF score : 0.283211\n",
      "Best mean score : 0.283247 + 0.010219 @ 196\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0360016836785 0.774348095059\n",
      "Fold  1 : 0.275689 @ 200 / best score is 0.275700 @ 199\n",
      "Fold  2 : 0.277560 @ 200 / best score is 0.277625 @ 198\n",
      "Fold  3 : 0.303141 @ 200 / best score is 0.303231 @ 195\n",
      "Fold  4 : 0.280975 @ 200 / best score is 0.281009 @ 197\n",
      "Fold  5 : 0.280431 @ 200 / best score is 0.280489 @ 196\n",
      "2.57894736842\n",
      "Full OOF score : 0.283499\n",
      "Best mean score : 0.283546 + 0.009903 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0360521771945 0.784914046526\n",
      "Fold  1 : 0.275185 @ 200 / best score is 0.275352 @ 167\n",
      "Fold  2 : 0.277683 @ 200 / best score is 0.277689 @ 198\n",
      "Fold  3 : 0.305210 @ 200 / best score is 0.305486 @ 188\n",
      "Fold  4 : 0.281559 @ 200 / best score is 0.281566 @ 190\n",
      "Fold  5 : 0.281085 @ 200 / best score is 0.281155 @ 196\n",
      "2.68421052632\n",
      "Full OOF score : 0.284075\n",
      "Best mean score : 0.284163 + 0.010734 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0377421132289 0.787538796663\n",
      "Fold  1 : 0.274401 @ 200 / best score is 0.274545 @ 198\n",
      "Fold  2 : 0.277447 @ 200 / best score is 0.277527 @ 197\n",
      "Fold  3 : 0.303200 @ 200 / best score is 0.303476 @ 195\n",
      "Fold  4 : 0.281550 @ 200 / best score is 0.281644 @ 193\n",
      "Fold  5 : 0.279689 @ 200 / best score is 0.279880 @ 196\n",
      "2.78947368421\n",
      "Full OOF score : 0.283186\n",
      "Best mean score : 0.283360 + 0.010278 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0384909925051 0.788250491023\n",
      "Fold  1 : 0.273532 @ 200 / best score is 0.273581 @ 177\n",
      "Fold  2 : 0.277645 @ 200 / best score is 0.277661 @ 199\n",
      "Fold  3 : 0.304478 @ 200 / best score is 0.304603 @ 198\n",
      "Fold  4 : 0.281062 @ 200 / best score is 0.281206 @ 187\n",
      "Fold  5 : 0.279037 @ 200 / best score is 0.279081 @ 196\n",
      "2.89473684211\n",
      "Full OOF score : 0.283073\n",
      "Best mean score : 0.283180 + 0.011008 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0396388270892 0.783033147454\n",
      "Fold  1 : 0.274477 @ 200 / best score is 0.274535 @ 168\n",
      "Fold  2 : 0.279333 @ 200 / best score is 0.279361 @ 198\n",
      "Fold  3 : 0.303856 @ 200 / best score is 0.303920 @ 197\n",
      "Fold  4 : 0.281081 @ 200 / best score is 0.281107 @ 196\n",
      "Fold  5 : 0.278367 @ 200 / best score is 0.278588 @ 196\n",
      "3.0\n",
      "Full OOF score : 0.283369\n",
      "Best mean score : 0.283455 + 0.010453 @ 198\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "0.0407337765209 0.796141862869\n",
      "[0.28239960950168397, 0.2828966903536855, 0.2833885009493211, 0.2822885242127464, 0.28341556401957146, 0.28345539089651406, 0.2835767566092793, 0.2841743213143133, 0.2840903199927569, 0.28356930083453535, 0.2835573619822648, 0.28338079617722733, 0.28445493264719257, 0.28376614730492955, 0.2832108592635463, 0.283499089208042, 0.2840747652938844, 0.28318613602894716, 0.28307342494324017, 0.2833685816435785]\n"
     ]
    }
   ],
   "source": [
    "target = train_label\n",
    "trn_df = pd.DataFrame(X.toarray())\n",
    "sub_df = pd.DataFrame(X_test.toarray())\n",
    "\n",
    "n_splits = 5\n",
    "n_estimators = 200\n",
    "\n",
    "final_cv_pred = np.zeros(len(sub_df))\n",
    "x_score = []\n",
    "\n",
    "# for sp in range(1,30, 1):\n",
    "for sp in np.linspace(1., 3., 20):\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=15) \n",
    "    imp_df = np.zeros((len(trn_df.columns), n_splits))\n",
    "    xgb_evals = np.zeros((n_estimators, n_splits))\n",
    "    oof = np.empty(len(trn_df))\n",
    "    sub_preds = np.zeros(len(sub_df))\n",
    "    increase = True\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n",
    "        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n",
    "        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n",
    "\n",
    "\n",
    "        # Upsample during cross validation to avoid having the same samples\n",
    "        # in both trn_df and validation sets\n",
    "        # Validation set is not up-sampled to monitor overfitting\n",
    "        if increase:\n",
    "            # Get positive examples\n",
    "            pos = pd.Series(trn_tgt == 1)\n",
    "            # Add positive examples\n",
    "            trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "            trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "            # Shuffle data\n",
    "            idx = np.arange(len(trn_dat))\n",
    "            np.random.shuffle(idx)\n",
    "            trn_dat = trn_dat.iloc[idx]\n",
    "            trn_tgt = trn_tgt.iloc[idx]\n",
    "\n",
    "    #     print(sum(trn_tgt==0)/sum(trn_tgt==1))\n",
    "\n",
    "    #     sp = sum(trn_tgt==0)/sum(trn_tgt==1)\n",
    "    #         sp = 1.55556\n",
    "#         sp = 1.52631578947\n",
    "#         clf = XGBClassifier(n_estimators=n_estimators,\n",
    "#                             max_depth=4,\n",
    "#                             objective=\"binary:logistic\",\n",
    "#                             learning_rate=.1, \n",
    "#                             subsample=.8, \n",
    "#                             colsample_bytree=.8,\n",
    "#                             scale_pos_weight=sp,\n",
    "#                             missing=-1,\n",
    "#                             gamma=1,\n",
    "#                             reg_alpha=0,\n",
    "#                             reg_lambda=1,\n",
    "#                             seed=s,\n",
    "#                             nthread=5)\n",
    "\n",
    "#         print(sp)\n",
    "        xgb_param_2 = {\n",
    "            'learning_rate': 0.07,\n",
    "            'objective':\"binary:logistic\",\n",
    "            'n_estimators':200,\n",
    "            'max_depth':4,\n",
    "            'min_child_weight':6,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'scale_pos_weight':sp,\n",
    "            'gamma':10,\n",
    "            'reg_alpha':8,\n",
    "            'reg_lambda':1.3,\n",
    "            'seed':99\n",
    "        }\n",
    "        clf = XGBClassifier(** xgb_param_2)\n",
    "        clf.fit(trn_dat, trn_tgt, \n",
    "                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n",
    "                eval_metric=gini_xgb,\n",
    "                early_stopping_rounds=None,\n",
    "                verbose=False)\n",
    "\n",
    "        # Keep feature importances\n",
    "        imp_df[:, fold_] = clf.feature_importances_\n",
    "\n",
    "        # Find best round for validation set\n",
    "        xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n",
    "        # Xgboost provides best round starting from 0 so it has to be incremented\n",
    "        best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n",
    "#         print(best_round)\n",
    "\n",
    "        # Predict OOF and submission probas with the best round\n",
    "        oof[val_idx] = clf.predict_proba(val_dat, ntree_limit=int(best_round))[:, 1]\n",
    "        # Update submission\n",
    "        sub_preds += clf.predict_proba(sub_df, ntree_limit=int(best_round))[:, 1] / n_splits\n",
    "\n",
    "        # Display results\n",
    "        print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n",
    "              % (fold_ + 1,\n",
    "                 eval_gini(val_tgt, oof[val_idx]),\n",
    "                 n_estimators,\n",
    "                 xgb_evals[best_round, fold_],\n",
    "                 best_round))\n",
    "\n",
    "    print(sp)\n",
    "    full_oof_score = eval_gini(target, oof)\n",
    "    print(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n",
    "    # Compute mean score and std\n",
    "    mean_eval = np.mean(xgb_evals, axis=1)\n",
    "    std_eval = np.std(xgb_evals, axis=1)\n",
    "    best_round = np.argsort(mean_eval)[::-1][0]\n",
    "\n",
    "    print(\"Best mean score : %.6f + %.6f @%4d\"\n",
    "          % (mean_eval[best_round], std_eval[best_round], best_round))\n",
    "    print('--------------------------------------')\n",
    "    print('\\n')\n",
    "    m1 = np.min(sub_preds)\n",
    "    m2 = np.max(sub_preds)\n",
    "    print(m1, m2)\n",
    "    assert((m1 > 0) & (m2 <= 1.))\n",
    "    final_cv_pred += sub_preds\n",
    "    \n",
    "    x_score.append(full_oof_score)\n",
    "print(x_score)\n",
    "pd.DataFrame({'id': test_id, 'target': final_cv_pred / 16.}).to_csv('../submissions/xgb3_pred_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35430169207357876\n"
     ]
    }
   ],
   "source": [
    "print(sum(x_score)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.49337462335825"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(final_cv_pred\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
