{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/yekenot/2-level-stacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.4 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from datetime import datetime\n",
    "from numba import jit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing (remove ps_calc cols, replace -1 with NaN, OHE categorical features)\n",
    "id_test = test['id'].values\n",
    "target_train = train['target'].values\n",
    "target = train['target']\n",
    "train = train.drop(['target','id'], axis = 1)\n",
    "test = test.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Original author CPMP : https://www.kaggle.com/cpmpml\n",
    "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    \"ps_car_11_cat\" # Very nice spot from Tilii : https://www.kaggle.com/tilii7\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_ind_reg = [\n",
    "    'ps_car_13',\n",
    "    'ps_reg_03'\n",
    "]\n",
    "\n",
    "# transformations = ['sq', 'sqrt', 'exp', 'div_sqrt', 'cbrt', 'pow_3', 'pow_5', 'sin', 'log']\n",
    "\n",
    "transformations = ['sin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans(t, x):\n",
    "    if (x == -1) | (float(x) == -1.0):\n",
    "        return -1\n",
    "    if float(x) == 0.0:\n",
    "        x = float(x) + 0.001 # increment x by delta\n",
    "    if t == 'sq':\n",
    "        return x * x\n",
    "    elif t == 'sqrt':\n",
    "        return math.sqrt(x)\n",
    "    elif t == 'exp':\n",
    "        return math.exp(x)\n",
    "    elif t == 'div_sqrt':\n",
    "        return 1./math.sqrt(x)\n",
    "    elif t == 'cbrt':\n",
    "        return x ** (1./3)\n",
    "    elif t == 'pow_3':\n",
    "        return 3 ** x\n",
    "    elif t == 'pow_5':\n",
    "        return 5 ** x\n",
    "    elif t == 'pow_5':\n",
    "        return 5 ** x\n",
    "    elif t == 'sin':\n",
    "        return math.sin(x)\n",
    "    elif t == 'log':\n",
    "        return math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train[name1] = train[f1].apply(lambda x: str(x)) + \"_\" + train[f2].apply(lambda x: str(x))\n",
    "    test[name1] = test[f1].apply(lambda x: str(x)) + \"_\" + test[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[name1].values) + list(test[name1].values))\n",
    "    train[name1] = lbl.transform(list(train[name1].values))\n",
    "    test[name1] = lbl.transform(list(test[name1].values))\n",
    "\n",
    "    train_features.append(name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['ps_car_13_x_ps_reg_03'] = train['ps_car_13'] * train['ps_reg_03']\n",
    "test['ps_car_13_x_ps_reg_03'] = test['ps_car_13'] * test['ps_reg_03']\n",
    "train_features.append('ps_car_13_x_ps_reg_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_calc_counts = ['ps_car_14','ps_car_13','ps_reg_03', 'ps_ind_03']\n",
    "f_calc_cats = ['ps_car_01_cat', 'ps_ind_05_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in f_calc_counts:\n",
    "    for f in f_calc_cats:\n",
    "\n",
    "        new_col1 = '{}_{}_mean'.format(col, f) \n",
    "        new_col2 = '{}_{}_median'.format(col, f) \n",
    "        new_col3 = '{}_{}_skew'.format(col, f) \n",
    "        new_col4 = '{}_{}_kurtosis'.format(col, f) \n",
    "        train[new_col1] = 0\n",
    "        train[new_col2] = 0\n",
    "        train[new_col3] = 0\n",
    "        train[new_col4] = 0\n",
    "        \n",
    "        test[new_col1] = 0\n",
    "        test[new_col2] = 0\n",
    "        test[new_col3] = 0\n",
    "        test[new_col4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:19<00:00, 34.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(f_calc_counts):\n",
    "    for f in f_calc_cats:\n",
    "        new_col1 = '{}_{}_mean'.format(col, f) \n",
    "        new_col2 = '{}_{}_median'.format(col, f) \n",
    "        new_col3 = '{}_{}_skew'.format(col, f) \n",
    "        new_col4 = '{}_{}_kurtosis'.format(col, f) \n",
    "        unique_f = np.unique(train[f].values)\n",
    "        for val in unique_f:\n",
    "            if val == -1:\n",
    "                continue\n",
    "            data1 = train[col][train[f] == val]\n",
    "            mean1 = data1.mean()\n",
    "            median1 = data1.median()\n",
    "            skew1 = data1.skew()\n",
    "            kurtosis1 = data1.kurtosis()\n",
    "            train[new_col1][train[f] == val] = mean1\n",
    "            train[new_col2][train[f] == val] = median1\n",
    "            train[new_col3][train[f] == val] = skew1\n",
    "            train[new_col4][train[f] == val] = kurtosis1\n",
    "            \n",
    "            data1 = test[col][test[f] == val]\n",
    "            mean1 = data1.mean()\n",
    "            median1 = data1.median()\n",
    "            skew1 = data1.skew()\n",
    "            kurtosis1 = data1.kurtosis()\n",
    "            test[new_col1][test[f] == val] = mean1\n",
    "            test[new_col2][test[f] == val] = median1\n",
    "            test[new_col3][test[f] == val] = skew1\n",
    "            test[new_col4][test[f] == val] = kurtosis1\n",
    "            \n",
    "            data2 = train[col][train[f] == val]\n",
    "            mean2 = data2.mean()\n",
    "            median2 = data2.median()\n",
    "            skew2 = data2.skew()\n",
    "            kurtosis2 = data1.kurtosis()\n",
    "            train[new_col1][train[f] == val] = mean2\n",
    "            train[new_col2][train[f] == val] = median2\n",
    "            train[new_col3][train[f] == val] = skew2\n",
    "            train[new_col4][train[f] == val] = kurtosis2\n",
    "            \n",
    "            data2 = test[col][test[f] == val]\n",
    "            mean2 = data2.mean()\n",
    "            median2 = data2.median()\n",
    "            skew2 = data2.skew()\n",
    "            kurtosis2 = data1.kurtosis()\n",
    "            test[new_col1][test[f] == val] = mean2\n",
    "            test[new_col2][test[f] == val] = median2\n",
    "            test[new_col3][test[f] == val] = skew2\n",
    "            test[new_col4][test[f] == val] = kurtosis2\n",
    "        \n",
    "        train_features.append(new_col1)\n",
    "        train_features.append(new_col2)\n",
    "        train_features.append(new_col3)\n",
    "        train_features.append(new_col4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train_features]\n",
    "test = test[train_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_cats = [f for f in train.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                         tst_series=test[f],\n",
    "                                         target=target,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "# train = train.drop(col_to_drop, axis=1)  \n",
    "# test = test.drop(col_to_drop, axis=1)  \n",
    "\n",
    "train = train.replace(-1, np.nan)\n",
    "test = test.replace(-1, np.nan)\n",
    "\n",
    "# cat_features = [a for a in train.columns if a.endswith('cat')]\n",
    "\n",
    "# for column in cat_features:\n",
    "#     temp = pd.get_dummies(pd.Series(train[column]))\n",
    "#     train = pd.concat([train,temp],axis=1)\n",
    "#     train = train.drop([column],axis=1)\n",
    "    \n",
    "# for column in cat_features:\n",
    "#     temp = pd.get_dummies(pd.Series(test[column]))\n",
    "#     test = pd.concat([test,temp],axis=1)\n",
    "#     test = test.drop([column],axis=1)\n",
    "\n",
    "# print(train.values.shape, test.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_score = 0.0\n",
    "class Ensemble(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        OOF_columns = []\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                S_train[test_idx, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]                \n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "            \n",
    "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\n",
    "            print(\"  Base model_%d gini score: %.5f\\n\" % (i+1, eval_gini(y, S_train[:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\n",
    "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                print(\"  1st level model_%d gini score: %.5f\\n\"%(i+1,\n",
    "                                                            eval_gini(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                print(\"  1st level model_%d gini score: %.5f\\n\"%(i+1,eval_gini(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "            print(\"2nd level model final gini score: %.5f\" % (eval_gini(y, stack_score)))\n",
    "            final_score = eval_gini(y, stack_score.mean(axis=1))    \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "            print(\"2nd level models final gini score: %.5f\" % (eval_gini(y, stack_score.mean(axis=1))))\n",
    "            final_score = eval_gini(y, stack_score.mean(axis=1))\n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgb_params_1 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 475,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 1200,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.3,  \n",
    "    'num_leaves': 16,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 750,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 500,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate': 0.025,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_5 = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_6 = {\n",
    "    'learning_rate': 0.035,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_7 = {\n",
    "    'learning_rate': 0.04,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 28,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_8 = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 25,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 700,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_9 = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 25,\n",
    "    'max_bin': 10,\n",
    "    'min_child_samples': 800,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "lgb_params_10 = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 550,\n",
    "    'subsample': 0.4,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.9,  \n",
    "    'num_leaves': 30,\n",
    "    'max_bin': 15,\n",
    "    'min_child_samples': 800,\n",
    "    'seed': 99\n",
    "}\n",
    "\n",
    "\n",
    "xgb_param_1 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "         \n",
    "xgb_param_2 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.6,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_3 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':26.43,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_4 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_5 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':2.0,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_6 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':2.0,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_7 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.53,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_8 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':26.43,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_9 = {\n",
    "    'learning_rate': 0.08,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_10 = {\n",
    "    'learning_rate': 0.09,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':200,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':10,\n",
    "    'reg_alpha':8,\n",
    "    'reg_lambda':1.3,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_11 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':4,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "\n",
    "xgb_param_12 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':4,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "\n",
    "xgb_param_13 = {\n",
    "    'learning_rate': 0.09,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_14 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':6,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_15 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':6,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_15 = {\n",
    "    'learning_rate': 0.07,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "xgb_param_15 = {\n",
    "    'learning_rate': 0.06,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'n_estimators':250,\n",
    "    'max_depth':5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'scale_pos_weight':1.52632,\n",
    "    'gamma':1,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'seed':99\n",
    "}\n",
    "\n",
    "rgf_param_1 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_2 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.02,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_3 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':.1,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_4 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':.4,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_5 = {\n",
    "    'max_leaf':3000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.09,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_6 = {\n",
    "    'max_leaf':2000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':10,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.07,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_7 = {\n",
    "    'max_leaf':2000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':5,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.07,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_8 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':5,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.08,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_9 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':5,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.08,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_10 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':15,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_11 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.01,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':20,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_12 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.01,\n",
    "    'sl2':0.02,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':20,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'n_jobs':-1,\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_13 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.02,\n",
    "    'sl2':0.02,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':20,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':100,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_14 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.02,\n",
    "    'sl2':0.02,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':20,\n",
    "    'n_iter':None,\n",
    "    'opt_interval':200,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "rgf_param_15 = {\n",
    "    'max_leaf':1000,\n",
    "    'algorithm':\"RGF\",  \n",
    "    'loss':\"Log\",\n",
    "    'l2':0.02,\n",
    "    'sl2':0.02,\n",
    "    'normalize':False,\n",
    "    'min_samples_leaf':5,\n",
    "    'n_iter':4,\n",
    "    'opt_interval':200,\n",
    "    'learning_rate':0.5,\n",
    "    'calc_prob':\"sigmoid\",\n",
    "    'memory_policy':\"generous\",\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "cb_param_1 = {\n",
    "    'iterations': 800,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.03,\n",
    "    'l2_leaf_reg':3.5,  \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99,\n",
    "    'verbose':0\n",
    "}\n",
    "\n",
    "cb_param_2 = {\n",
    "    'iterations': 800,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.03,\n",
    "    'l2_leaf_reg':4.0 , \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_3 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.05,\n",
    "    'l2_leaf_reg':3.5,  \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_4 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.07,\n",
    "    'l2_leaf_reg':3.5,  \n",
    "    'border_count':10,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_5 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.07,\n",
    "    'l2_leaf_reg':3.5,  \n",
    "    'border_count':10,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_6 = {\n",
    "    'iterations': 700,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.06,\n",
    "    'l2_leaf_reg':3.5, \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_7 = {\n",
    "    'iterations': 700,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.06,\n",
    "    'l2_leaf_reg':3.5 , \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':5,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_8 = {\n",
    "    'iterations': 700,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.05,\n",
    "    'l2_leaf_reg':3.5 , \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_9 = {\n",
    "    'iterations': 700,\n",
    "    'depth':8,\n",
    "    'rsm':0.95,\n",
    "    'learning_rate':0.05,\n",
    "    'l2_leaf_reg':3.5 , \n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_10 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.85,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_11 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.75,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_12 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.80,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_13 = {\n",
    "    'iterations': 900,\n",
    "    'depth':8,\n",
    "    'rsm':0.90,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_14 = {\n",
    "    'iterations': 900,\n",
    "    'depth':4,\n",
    "    'rsm':0.85,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20,\n",
    "    'random_seed':99\n",
    "}\n",
    "\n",
    "cb_param_15 = {\n",
    "    'iterations': 900,\n",
    "    'depth':5,\n",
    "    'rsm':0.85,\n",
    "    'learning_rate':0.02,\n",
    "    'l2_leaf_reg':3.5  ,\n",
    "    'border_count':8,\n",
    "    'gradient_iterations':4,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':100,\n",
    "    'random_seed':99\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Base models\n",
    "lgb_model_1 = LGBMClassifier(**lgb_params_1)\n",
    "\n",
    "lgb_model_2 = LGBMClassifier(**lgb_params_2)\n",
    "\n",
    "lgb_model_3 = LGBMClassifier(**lgb_params_3)\n",
    "\n",
    "lgb_model_4 = LGBMClassifier(**lgb_params_4)\n",
    "\n",
    "lgb_model_5 = LGBMClassifier(**lgb_params_5)\n",
    "\n",
    "lgb_model_6 = LGBMClassifier(**lgb_params_6)\n",
    "\n",
    "lgb_model_7 = LGBMClassifier(**lgb_params_7)\n",
    "\n",
    "lgb_model_8 = LGBMClassifier(**lgb_params_8)\n",
    "\n",
    "lgb_model_9 = LGBMClassifier(**lgb_params_9)\n",
    "\n",
    "lgb_model_10 = LGBMClassifier(**lgb_params_10)\n",
    "\n",
    "xgb_model_1 = XGBClassifier(**xgb_param_1)\n",
    "\n",
    "xgb_model_2 = XGBClassifier(**xgb_param_2)\n",
    "\n",
    "xgb_model_3 = XGBClassifier(**xgb_param_3)\n",
    "\n",
    "xgb_model_4 = XGBClassifier(**xgb_param_4)\n",
    "\n",
    "xgb_model_5 = XGBClassifier(**xgb_param_5)\n",
    "\n",
    "xgb_model_6 = XGBClassifier(**xgb_param_6)\n",
    "\n",
    "xgb_model_7 = XGBClassifier(**xgb_param_7)\n",
    "\n",
    "xgb_model_8 = XGBClassifier(**xgb_param_8)\n",
    "\n",
    "xgb_model_9 = XGBClassifier(**xgb_param_9)\n",
    "\n",
    "xgb_model_10 = XGBClassifier(**xgb_param_10)\n",
    "\n",
    "xgb_model_11 = XGBClassifier(**xgb_param_11)\n",
    "\n",
    "xgb_model_12 = XGBClassifier(**xgb_param_12)\n",
    "\n",
    "xgb_model_13 = XGBClassifier(**xgb_param_13)\n",
    "\n",
    "xgb_model_14 = XGBClassifier(**xgb_param_14)\n",
    "\n",
    "xgb_model_15 = XGBClassifier(**xgb_param_15)\n",
    "\n",
    "rgf_model_1 = RGFClassifier(** rgf_param_1)\n",
    "\n",
    "rgf_model_2 = RGFClassifier(** rgf_param_2)\n",
    "\n",
    "rgf_model_3 = RGFClassifier(** rgf_param_3)\n",
    "\n",
    "rgf_model_4 = RGFClassifier(** rgf_param_4)\n",
    "\n",
    "rgf_model_5 = RGFClassifier(** rgf_param_5)\n",
    "\n",
    "rgf_model_6 = RGFClassifier(** rgf_param_6)\n",
    "\n",
    "rgf_model_7 = RGFClassifier(** rgf_param_7)\n",
    "\n",
    "rgf_model_8 = RGFClassifier(** rgf_param_8)\n",
    "\n",
    "rgf_model_9 = RGFClassifier(** rgf_param_9)\n",
    "\n",
    "rgf_model_10 = RGFClassifier(** rgf_param_10)\n",
    "\n",
    "cb_model_1 = CatBoostClassifier(** cb_param_1)\n",
    "\n",
    "cb_model_2 = CatBoostClassifier(** cb_param_2)\n",
    "\n",
    "cb_model_3 = CatBoostClassifier(** cb_param_3)\n",
    "\n",
    "cb_model_4 = CatBoostClassifier(** cb_param_4)\n",
    "\n",
    "cb_model_5 = CatBoostClassifier(** cb_param_5)\n",
    "\n",
    "cb_model_6 = CatBoostClassifier(** cb_param_6)\n",
    "\n",
    "cb_model_7 = CatBoostClassifier(** cb_param_7)\n",
    "\n",
    "cb_model_8 = CatBoostClassifier(** cb_param_8)\n",
    "\n",
    "cb_model_9 = CatBoostClassifier(** cb_param_9)\n",
    "\n",
    "cb_model_10 = CatBoostClassifier(** cb_param_10)\n",
    "\n",
    "cb_model_11 = CatBoostClassifier(** cb_param_11)\n",
    "\n",
    "cb_model_12 = CatBoostClassifier(** cb_param_12)\n",
    "\n",
    "cb_model_13 = CatBoostClassifier(** cb_param_13)\n",
    "\n",
    "cb_model_14 = CatBoostClassifier(** cb_param_14)\n",
    "\n",
    "cb_model_15 = CatBoostClassifier(** cb_param_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacker models\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "et_model = ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_split=10, random_state=10)\n",
    "\n",
    "et_model2 = ExtraTreesClassifier(n_estimators=300, max_depth=6, min_samples_split=20, random_state=10)\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier_1 fold 1\n",
      "Fit LGBMClassifier_1 fold 2\n",
      "Fit LGBMClassifier_1 fold 3\n",
      "Fit LGBMClassifier_1 fold 4\n",
      "Fit LGBMClassifier_1 fold 5\n",
      "  Base model_1 score: 0.64310\n",
      "\n",
      "  Base model_1 gini score: 0.28621\n",
      "\n",
      "Fit LGBMClassifier_2 fold 1\n",
      "Fit LGBMClassifier_2 fold 2\n",
      "Fit LGBMClassifier_2 fold 3\n",
      "Fit LGBMClassifier_2 fold 4\n",
      "Fit LGBMClassifier_2 fold 5\n",
      "  Base model_2 score: 0.64198\n",
      "\n",
      "  Base model_2 gini score: 0.28395\n",
      "\n",
      "Fit LGBMClassifier_3 fold 1\n",
      "Fit LGBMClassifier_3 fold 2\n",
      "Fit LGBMClassifier_3 fold 3\n",
      "Fit LGBMClassifier_3 fold 4\n",
      "Fit LGBMClassifier_3 fold 5\n",
      "  Base model_3 score: 0.64247\n",
      "\n",
      "  Base model_3 gini score: 0.28494\n",
      "\n",
      "Fit LGBMClassifier_4 fold 1\n",
      "Fit LGBMClassifier_4 fold 2\n",
      "Fit LGBMClassifier_4 fold 3\n",
      "Fit LGBMClassifier_4 fold 4\n",
      "Fit LGBMClassifier_4 fold 5\n",
      "  Base model_4 score: 0.64343\n",
      "\n",
      "  Base model_4 gini score: 0.28685\n",
      "\n",
      "Fit LGBMClassifier_5 fold 1\n",
      "Fit LGBMClassifier_5 fold 2\n",
      "Fit LGBMClassifier_5 fold 3\n",
      "Fit LGBMClassifier_5 fold 4\n",
      "Fit LGBMClassifier_5 fold 5\n",
      "  Base model_5 score: 0.64200\n",
      "\n",
      "  Base model_5 gini score: 0.28400\n",
      "\n",
      "Fit LGBMClassifier_6 fold 1\n",
      "Fit LGBMClassifier_6 fold 2\n",
      "Fit LGBMClassifier_6 fold 3\n",
      "Fit LGBMClassifier_6 fold 4\n",
      "Fit LGBMClassifier_6 fold 5\n",
      "  Base model_6 score: 0.64113\n",
      "\n",
      "  Base model_6 gini score: 0.28226\n",
      "\n",
      "Fit LGBMClassifier_7 fold 1\n",
      "Fit LGBMClassifier_7 fold 2\n",
      "Fit LGBMClassifier_7 fold 3\n",
      "Fit LGBMClassifier_7 fold 4\n",
      "Fit LGBMClassifier_7 fold 5\n",
      "  Base model_7 score: 0.63904\n",
      "\n",
      "  Base model_7 gini score: 0.27809\n",
      "\n",
      "Fit LGBMClassifier_8 fold 1\n",
      "Fit LGBMClassifier_8 fold 2\n",
      "Fit LGBMClassifier_8 fold 3\n",
      "Fit LGBMClassifier_8 fold 4\n",
      "Fit LGBMClassifier_8 fold 5\n",
      "  Base model_8 score: 0.64282\n",
      "\n",
      "  Base model_8 gini score: 0.28563\n",
      "\n",
      "Fit LGBMClassifier_9 fold 1\n",
      "Fit LGBMClassifier_9 fold 2\n",
      "Fit LGBMClassifier_9 fold 3\n",
      "Fit LGBMClassifier_9 fold 4\n",
      "Fit LGBMClassifier_9 fold 5\n",
      "  Base model_9 score: 0.64312\n",
      "\n",
      "  Base model_9 gini score: 0.28625\n",
      "\n",
      "Fit LGBMClassifier_10 fold 1\n",
      "Fit LGBMClassifier_10 fold 2\n",
      "Fit LGBMClassifier_10 fold 3\n",
      "Fit LGBMClassifier_10 fold 4\n",
      "Fit LGBMClassifier_10 fold 5\n",
      "  Base model_10 score: 0.64131\n",
      "\n",
      "  Base model_10 gini score: 0.28261\n",
      "\n",
      "Fit XGBClassifier_11 fold 1\n",
      "Fit XGBClassifier_11 fold 2\n",
      "Fit XGBClassifier_11 fold 3\n",
      "Fit XGBClassifier_11 fold 4\n",
      "Fit XGBClassifier_11 fold 5\n",
      "  Base model_11 score: 0.64240\n",
      "\n",
      "  Base model_11 gini score: 0.28480\n",
      "\n",
      "Fit XGBClassifier_12 fold 1\n",
      "Fit XGBClassifier_12 fold 2\n",
      "Fit XGBClassifier_12 fold 3\n",
      "Fit XGBClassifier_12 fold 4\n",
      "Fit XGBClassifier_12 fold 5\n",
      "  Base model_12 score: 0.64255\n",
      "\n",
      "  Base model_12 gini score: 0.28510\n",
      "\n",
      "Fit XGBClassifier_13 fold 1\n",
      "Fit XGBClassifier_13 fold 2\n",
      "Fit XGBClassifier_13 fold 3\n",
      "Fit XGBClassifier_13 fold 4\n",
      "Fit XGBClassifier_13 fold 5\n",
      "  Base model_13 score: 0.63979\n",
      "\n",
      "  Base model_13 gini score: 0.27958\n",
      "\n",
      "Fit XGBClassifier_14 fold 1\n",
      "Fit XGBClassifier_14 fold 2\n",
      "Fit XGBClassifier_14 fold 3\n",
      "Fit XGBClassifier_14 fold 4\n",
      "Fit XGBClassifier_14 fold 5\n",
      "  Base model_14 score: 0.64250\n",
      "\n",
      "  Base model_14 gini score: 0.28501\n",
      "\n",
      "Fit XGBClassifier_15 fold 1\n",
      "Fit XGBClassifier_15 fold 2\n",
      "Fit XGBClassifier_15 fold 3\n",
      "Fit XGBClassifier_15 fold 4\n",
      "Fit XGBClassifier_15 fold 5\n",
      "  Base model_15 score: 0.64275\n",
      "\n",
      "  Base model_15 gini score: 0.28549\n",
      "\n",
      "Fit XGBClassifier_16 fold 1\n",
      "Fit XGBClassifier_16 fold 2\n",
      "Fit XGBClassifier_16 fold 3\n",
      "Fit XGBClassifier_16 fold 4\n",
      "Fit XGBClassifier_16 fold 5\n",
      "  Base model_16 score: 0.64281\n",
      "\n",
      "  Base model_16 gini score: 0.28562\n",
      "\n",
      "Fit XGBClassifier_17 fold 1\n",
      "Fit XGBClassifier_17 fold 2\n",
      "Fit XGBClassifier_17 fold 3\n",
      "Fit XGBClassifier_17 fold 4\n",
      "Fit XGBClassifier_17 fold 5\n",
      "  Base model_17 score: 0.64250\n",
      "\n",
      "  Base model_17 gini score: 0.28499\n",
      "\n",
      "Fit XGBClassifier_18 fold 1\n",
      "Fit XGBClassifier_18 fold 2\n",
      "Fit XGBClassifier_18 fold 3\n",
      "Fit XGBClassifier_18 fold 4\n",
      "Fit XGBClassifier_18 fold 5\n",
      "  Base model_18 score: 0.64181\n",
      "\n",
      "  Base model_18 gini score: 0.28362\n",
      "\n",
      "Fit XGBClassifier_19 fold 1\n",
      "Fit XGBClassifier_19 fold 2\n",
      "Fit XGBClassifier_19 fold 3\n",
      "Fit XGBClassifier_19 fold 4\n",
      "Fit XGBClassifier_19 fold 5\n",
      "  Base model_19 score: 0.64292\n",
      "\n",
      "  Base model_19 gini score: 0.28585\n",
      "\n",
      "Fit XGBClassifier_20 fold 1\n",
      "Fit XGBClassifier_20 fold 2\n",
      "Fit XGBClassifier_20 fold 3\n",
      "Fit XGBClassifier_20 fold 4\n",
      "Fit XGBClassifier_20 fold 5\n",
      "  Base model_20 score: 0.64335\n",
      "\n",
      "  Base model_20 gini score: 0.28670\n",
      "\n",
      "Fit XGBClassifier_21 fold 1\n",
      "Fit XGBClassifier_21 fold 2\n",
      "Fit XGBClassifier_21 fold 3\n",
      "Fit XGBClassifier_21 fold 4\n",
      "Fit XGBClassifier_21 fold 5\n",
      "  Base model_21 score: 0.64188\n",
      "\n",
      "  Base model_21 gini score: 0.28375\n",
      "\n",
      "Fit XGBClassifier_22 fold 1\n",
      "Fit XGBClassifier_22 fold 2\n",
      "Fit XGBClassifier_22 fold 3\n",
      "Fit XGBClassifier_22 fold 4\n",
      "Fit XGBClassifier_22 fold 5\n",
      "  Base model_22 score: 0.64167\n",
      "\n",
      "  Base model_22 gini score: 0.28333\n",
      "\n",
      "Fit XGBClassifier_23 fold 1\n",
      "Fit XGBClassifier_23 fold 2\n",
      "Fit XGBClassifier_23 fold 3\n",
      "Fit XGBClassifier_23 fold 4\n",
      "Fit XGBClassifier_23 fold 5\n",
      "  Base model_23 score: 0.63767\n",
      "\n",
      "  Base model_23 gini score: 0.27534\n",
      "\n",
      "Fit XGBClassifier_24 fold 1\n",
      "Fit XGBClassifier_24 fold 2\n",
      "Fit XGBClassifier_24 fold 3\n",
      "Fit XGBClassifier_24 fold 4\n",
      "Fit XGBClassifier_24 fold 5\n",
      "  Base model_24 score: 0.63988\n",
      "\n",
      "  Base model_24 gini score: 0.27976\n",
      "\n",
      "Fit XGBClassifier_25 fold 1\n",
      "Fit XGBClassifier_25 fold 2\n",
      "Fit XGBClassifier_25 fold 3\n",
      "Fit XGBClassifier_25 fold 4\n",
      "Fit XGBClassifier_25 fold 5\n",
      "  Base model_25 score: 0.64281\n",
      "\n",
      "  Base model_25 gini score: 0.28561\n",
      "\n",
      "\n",
      "\n",
      "Correlation between out-of-fold predictions from Base models:\n",
      "\n",
      "\n",
      "               Base model_1  Base model_2  Base model_3  Base model_4  \\\n",
      "Base model_1       1.000000      0.958327      0.972752      0.980974   \n",
      "Base model_2       0.958327      1.000000      0.954472      0.958436   \n",
      "Base model_3       0.972752      0.954472      1.000000      0.980606   \n",
      "Base model_4       0.980974      0.958436      0.980606      1.000000   \n",
      "Base model_5       0.971924      0.946855      0.965954      0.972634   \n",
      "Base model_6       0.960682      0.938002      0.958519      0.964546   \n",
      "Base model_7       0.948688      0.927453      0.949504      0.955218   \n",
      "Base model_8       0.975104      0.950128      0.968031      0.974879   \n",
      "Base model_9       0.975125      0.948761      0.967495      0.974720   \n",
      "Base model_10      0.961507      0.948474      0.956069      0.962748   \n",
      "Base model_11      0.941768      0.950662      0.937154      0.942457   \n",
      "Base model_12      0.968138      0.961565      0.954067      0.961344   \n",
      "Base model_13      0.875371      0.868131      0.864288      0.870317   \n",
      "Base model_14      0.956762      0.959572      0.947656      0.953590   \n",
      "Base model_15      0.959464      0.960448      0.949671      0.955699   \n",
      "Base model_16      0.968092      0.962579      0.956001      0.962977   \n",
      "Base model_17      0.968153      0.961100      0.953553      0.960942   \n",
      "Base model_18      0.897307      0.886771      0.883147      0.889789   \n",
      "Base model_19      0.965971      0.961230      0.953588      0.961085   \n",
      "Base model_20      0.964538      0.960924      0.953438      0.960839   \n",
      "Base model_21      0.934193      0.945448      0.933143      0.937634   \n",
      "Base model_22      0.933708      0.944341      0.932568      0.937112   \n",
      "Base model_23      0.877456      0.895390      0.886026      0.888077   \n",
      "Base model_24      0.912278      0.925891      0.918104      0.920416   \n",
      "Base model_25      0.944288      0.952434      0.943594      0.947582   \n",
      "\n",
      "               Base model_5  Base model_6  Base model_7  Base model_8  \\\n",
      "Base model_1       0.971924      0.960682      0.948688      0.975104   \n",
      "Base model_2       0.946855      0.938002      0.927453      0.950128   \n",
      "Base model_3       0.965954      0.958519      0.949504      0.968031   \n",
      "Base model_4       0.972634      0.964546      0.955218      0.974879   \n",
      "Base model_5       1.000000      0.976080      0.966489      0.986737   \n",
      "Base model_6       0.976080      1.000000      0.969121      0.975325   \n",
      "Base model_7       0.966489      0.969121      1.000000      0.965325   \n",
      "Base model_8       0.986737      0.975325      0.965325      1.000000   \n",
      "Base model_9       0.984982      0.974707      0.964939      0.986480   \n",
      "Base model_10      0.966401      0.960095      0.952447      0.967036   \n",
      "Base model_11      0.927519      0.918306      0.907012      0.930973   \n",
      "Base model_12      0.944162      0.932100      0.918341      0.948358   \n",
      "Base model_13      0.859885      0.850306      0.839829      0.862702   \n",
      "Base model_14      0.936534      0.925696      0.912803      0.940573   \n",
      "Base model_15      0.939046      0.927754      0.915069      0.943038   \n",
      "Base model_16      0.946596      0.935176      0.922283      0.950498   \n",
      "Base model_17      0.943387      0.931261      0.917442      0.947715   \n",
      "Base model_18      0.877683      0.866912      0.855314      0.880807   \n",
      "Base model_19      0.944240      0.932633      0.919461      0.948237   \n",
      "Base model_20      0.944774      0.933387      0.920795      0.948686   \n",
      "Base model_21      0.923331      0.914979      0.904559      0.926537   \n",
      "Base model_22      0.921592      0.913169      0.902260      0.924886   \n",
      "Base model_23      0.879681      0.875703      0.869718      0.880719   \n",
      "Base model_24      0.908761      0.902848      0.894464      0.910312   \n",
      "Base model_25      0.931941      0.923417      0.912522      0.934997   \n",
      "\n",
      "               Base model_9  Base model_10      ...        Base model_16  \\\n",
      "Base model_1       0.975125       0.961507      ...             0.968092   \n",
      "Base model_2       0.948761       0.948474      ...             0.962579   \n",
      "Base model_3       0.967495       0.956069      ...             0.956001   \n",
      "Base model_4       0.974720       0.962748      ...             0.962977   \n",
      "Base model_5       0.984982       0.966401      ...             0.946596   \n",
      "Base model_6       0.974707       0.960095      ...             0.935176   \n",
      "Base model_7       0.964939       0.952447      ...             0.922283   \n",
      "Base model_8       0.986480       0.967036      ...             0.950498   \n",
      "Base model_9       1.000000       0.968030      ...             0.950702   \n",
      "Base model_10      0.968030       1.000000      ...             0.948505   \n",
      "Base model_11      0.930610       0.931584      ...             0.966813   \n",
      "Base model_12      0.948334       0.945573      ...             0.991092   \n",
      "Base model_13      0.864724       0.861393      ...             0.894554   \n",
      "Base model_14      0.940061       0.938769      ...             0.982686   \n",
      "Base model_15      0.942783       0.941036      ...             0.984715   \n",
      "Base model_16      0.950702       0.948505      ...             1.000000   \n",
      "Base model_17      0.947646       0.944509      ...             0.990797   \n",
      "Base model_18      0.882803       0.878699      ...             0.916194   \n",
      "Base model_19      0.947924       0.946089      ...             0.987963   \n",
      "Base model_20      0.948362       0.947028      ...             0.986282   \n",
      "Base model_21      0.926083       0.926967      ...             0.958301   \n",
      "Base model_22      0.924332       0.925198      ...             0.958543   \n",
      "Base model_23      0.880386       0.886245      ...             0.896333   \n",
      "Base model_24      0.909994       0.913441      ...             0.931406   \n",
      "Base model_25      0.934519       0.935109      ...             0.965530   \n",
      "\n",
      "               Base model_17  Base model_18  Base model_19  Base model_20  \\\n",
      "Base model_1        0.968153       0.897307       0.965971       0.964538   \n",
      "Base model_2        0.961100       0.886771       0.961230       0.960924   \n",
      "Base model_3        0.953553       0.883147       0.953588       0.953438   \n",
      "Base model_4        0.960942       0.889789       0.961085       0.960839   \n",
      "Base model_5        0.943387       0.877683       0.944240       0.944774   \n",
      "Base model_6        0.931261       0.866912       0.932633       0.933387   \n",
      "Base model_7        0.917442       0.855314       0.919461       0.920795   \n",
      "Base model_8        0.947715       0.880807       0.948237       0.948686   \n",
      "Base model_9        0.947646       0.882803       0.947924       0.948362   \n",
      "Base model_10       0.944509       0.878699       0.946089       0.947028   \n",
      "Base model_11       0.962998       0.890576       0.964521       0.965678   \n",
      "Base model_12       0.993541       0.908439       0.988827       0.986678   \n",
      "Base model_13       0.885079       0.971206       0.884725       0.883969   \n",
      "Base model_14       0.982388       0.899848       0.979953       0.978579   \n",
      "Base model_15       0.982596       0.911630       0.980609       0.979241   \n",
      "Base model_16       0.990797       0.916194       0.987963       0.986282   \n",
      "Base model_17       1.000000       0.907876       0.988759       0.986614   \n",
      "Base model_18       0.907876       1.000000       0.906119       0.904630   \n",
      "Base model_19       0.988759       0.906119       1.000000       0.989313   \n",
      "Base model_20       0.986614       0.904630       0.989313       1.000000   \n",
      "Base model_21       0.954202       0.883500       0.957434       0.958323   \n",
      "Base model_22       0.955192       0.883499       0.958014       0.958491   \n",
      "Base model_23       0.887320       0.835428       0.892687       0.895360   \n",
      "Base model_24       0.924503       0.865298       0.928350       0.930109   \n",
      "Base model_25       0.961717       0.890172       0.964059       0.964505   \n",
      "\n",
      "               Base model_21  Base model_22  Base model_23  Base model_24  \\\n",
      "Base model_1        0.934193       0.933708       0.877456       0.912278   \n",
      "Base model_2        0.945448       0.944341       0.895390       0.925891   \n",
      "Base model_3        0.933143       0.932568       0.886026       0.918104   \n",
      "Base model_4        0.937634       0.937112       0.888077       0.920416   \n",
      "Base model_5        0.923331       0.921592       0.879681       0.908761   \n",
      "Base model_6        0.914979       0.913169       0.875703       0.902848   \n",
      "Base model_7        0.904559       0.902260       0.869718       0.894464   \n",
      "Base model_8        0.926537       0.924886       0.880719       0.910312   \n",
      "Base model_9        0.926083       0.924332       0.880386       0.909994   \n",
      "Base model_10       0.926967       0.925198       0.886245       0.913441   \n",
      "Base model_11       0.959863       0.958785       0.911540       0.936123   \n",
      "Base model_12       0.955381       0.955917       0.889986       0.926335   \n",
      "Base model_13       0.870207       0.869535       0.829020       0.854674   \n",
      "Base model_14       0.962068       0.962502       0.902217       0.935243   \n",
      "Base model_15       0.962264       0.962778       0.902595       0.935914   \n",
      "Base model_16       0.958301       0.958543       0.896333       0.931406   \n",
      "Base model_17       0.954202       0.955192       0.887320       0.924503   \n",
      "Base model_18       0.883500       0.883499       0.835428       0.865298   \n",
      "Base model_19       0.957434       0.958014       0.892687       0.928350   \n",
      "Base model_20       0.958323       0.958491       0.895360       0.930109   \n",
      "Base model_21       1.000000       0.965955       0.911167       0.942543   \n",
      "Base model_22       0.965955       1.000000       0.906468       0.939233   \n",
      "Base model_23       0.911167       0.906468       1.000000       0.916367   \n",
      "Base model_24       0.942543       0.939233       0.916367       1.000000   \n",
      "Base model_25       0.965946       0.965370       0.916094       0.952062   \n",
      "\n",
      "               Base model_25  \n",
      "Base model_1        0.944288  \n",
      "Base model_2        0.952434  \n",
      "Base model_3        0.943594  \n",
      "Base model_4        0.947582  \n",
      "Base model_5        0.931941  \n",
      "Base model_6        0.923417  \n",
      "Base model_7        0.912522  \n",
      "Base model_8        0.934997  \n",
      "Base model_9        0.934519  \n",
      "Base model_10       0.935109  \n",
      "Base model_11       0.963201  \n",
      "Base model_12       0.962869  \n",
      "Base model_13       0.874494  \n",
      "Base model_14       0.968694  \n",
      "Base model_15       0.968447  \n",
      "Base model_16       0.965530  \n",
      "Base model_17       0.961717  \n",
      "Base model_18       0.890172  \n",
      "Base model_19       0.964059  \n",
      "Base model_20       0.964505  \n",
      "Base model_21       0.965946  \n",
      "Base model_22       0.965370  \n",
      "Base model_23       0.916094  \n",
      "Base model_24       0.952062  \n",
      "Base model_25       1.000000  \n",
      "\n",
      "[25 rows x 25 columns]\n",
      "\n",
      "\n",
      "Fit LogisticRegression_1 subset 1\n",
      "Fit LogisticRegression_1 subset 2\n",
      "Fit LogisticRegression_1 subset 3\n",
      "Fit LogisticRegression_1 subset 4\n",
      "Fit LogisticRegression_1 subset 5\n",
      "Fit LogisticRegression_1 subset 6\n",
      "Fit LogisticRegression_1 subset 7\n",
      "Fit LogisticRegression_1 subset 8\n",
      "Fit LogisticRegression_1 subset 9\n",
      "Fit LogisticRegression_1 subset 10\n",
      "Fit LogisticRegression_1 subset 11\n",
      "Fit LogisticRegression_1 subset 12\n",
      "Fit LogisticRegression_1 subset 13\n",
      "Fit LogisticRegression_1 subset 14\n",
      "Fit LogisticRegression_1 subset 15\n",
      "Fit LogisticRegression_1 subset 16\n",
      "Fit LogisticRegression_1 subset 17\n",
      "Fit LogisticRegression_1 subset 18\n",
      "Fit LogisticRegression_1 subset 19\n",
      "Fit LogisticRegression_1 subset 20\n",
      "Fit LogisticRegression_1 subset 21\n",
      "Fit LogisticRegression_1 subset 22\n",
      "Fit LogisticRegression_1 subset 23\n",
      "Fit LogisticRegression_1 subset 24\n",
      "Fit LogisticRegression_1 subset 25\n",
      "  1st level model_1 score: 0.64328\n",
      "\n",
      "  1st level model_1 gini score: 0.28656\n",
      "\n",
      "Fit ExtraTreesClassifier_2 subset 1\n",
      "Fit ExtraTreesClassifier_2 subset 2\n",
      "Fit ExtraTreesClassifier_2 subset 3\n",
      "Fit ExtraTreesClassifier_2 subset 4\n",
      "Fit ExtraTreesClassifier_2 subset 5\n",
      "Fit ExtraTreesClassifier_2 subset 6\n",
      "Fit ExtraTreesClassifier_2 subset 7\n",
      "Fit ExtraTreesClassifier_2 subset 8\n",
      "Fit ExtraTreesClassifier_2 subset 9\n",
      "Fit ExtraTreesClassifier_2 subset 10\n",
      "Fit ExtraTreesClassifier_2 subset 11\n",
      "Fit ExtraTreesClassifier_2 subset 12\n",
      "Fit ExtraTreesClassifier_2 subset 13\n",
      "Fit ExtraTreesClassifier_2 subset 14\n",
      "Fit ExtraTreesClassifier_2 subset 15\n",
      "Fit ExtraTreesClassifier_2 subset 16\n",
      "Fit ExtraTreesClassifier_2 subset 17\n",
      "Fit ExtraTreesClassifier_2 subset 18\n",
      "Fit ExtraTreesClassifier_2 subset 19\n",
      "Fit ExtraTreesClassifier_2 subset 20\n",
      "Fit ExtraTreesClassifier_2 subset 21\n",
      "Fit ExtraTreesClassifier_2 subset 22\n",
      "Fit ExtraTreesClassifier_2 subset 23\n",
      "Fit ExtraTreesClassifier_2 subset 24\n",
      "Fit ExtraTreesClassifier_2 subset 25\n",
      "  1st level model_2 score: 0.64512\n",
      "\n",
      "  1st level model_2 gini score: 0.29024\n",
      "\n",
      "Fit ExtraTreesClassifier_3 subset 1\n",
      "Fit ExtraTreesClassifier_3 subset 2\n",
      "Fit ExtraTreesClassifier_3 subset 3\n",
      "Fit ExtraTreesClassifier_3 subset 4\n",
      "Fit ExtraTreesClassifier_3 subset 5\n",
      "Fit ExtraTreesClassifier_3 subset 6\n",
      "Fit ExtraTreesClassifier_3 subset 7\n",
      "Fit ExtraTreesClassifier_3 subset 8\n",
      "Fit ExtraTreesClassifier_3 subset 9\n",
      "Fit ExtraTreesClassifier_3 subset 10\n",
      "Fit ExtraTreesClassifier_3 subset 11\n",
      "Fit ExtraTreesClassifier_3 subset 12\n",
      "Fit ExtraTreesClassifier_3 subset 13\n",
      "Fit ExtraTreesClassifier_3 subset 14\n",
      "Fit ExtraTreesClassifier_3 subset 15\n",
      "Fit ExtraTreesClassifier_3 subset 16\n",
      "Fit ExtraTreesClassifier_3 subset 17\n",
      "Fit ExtraTreesClassifier_3 subset 18\n",
      "Fit ExtraTreesClassifier_3 subset 19\n",
      "Fit ExtraTreesClassifier_3 subset 20\n",
      "Fit ExtraTreesClassifier_3 subset 21\n",
      "Fit ExtraTreesClassifier_3 subset 22\n",
      "Fit ExtraTreesClassifier_3 subset 23\n",
      "Fit ExtraTreesClassifier_3 subset 24\n",
      "Fit ExtraTreesClassifier_3 subset 25\n",
      "  1st level model_3 score: 0.64553\n",
      "\n",
      "  1st level model_3 gini score: 0.29106\n",
      "\n",
      "Fit MLPClassifier_4 subset 1\n",
      "Fit MLPClassifier_4 subset 2\n",
      "Fit MLPClassifier_4 subset 3\n",
      "Fit MLPClassifier_4 subset 4\n",
      "Fit MLPClassifier_4 subset 5\n",
      "Fit MLPClassifier_4 subset 6\n",
      "Fit MLPClassifier_4 subset 7\n",
      "Fit MLPClassifier_4 subset 8\n",
      "Fit MLPClassifier_4 subset 9\n",
      "Fit MLPClassifier_4 subset 10\n",
      "Fit MLPClassifier_4 subset 11\n",
      "Fit MLPClassifier_4 subset 12\n",
      "Fit MLPClassifier_4 subset 13\n",
      "Fit MLPClassifier_4 subset 14\n",
      "Fit MLPClassifier_4 subset 15\n",
      "Fit MLPClassifier_4 subset 16\n",
      "Fit MLPClassifier_4 subset 17\n",
      "Fit MLPClassifier_4 subset 18\n",
      "Fit MLPClassifier_4 subset 19\n",
      "Fit MLPClassifier_4 subset 20\n",
      "Fit MLPClassifier_4 subset 21\n",
      "Fit MLPClassifier_4 subset 22\n",
      "Fit MLPClassifier_4 subset 23\n",
      "Fit MLPClassifier_4 subset 24\n",
      "Fit MLPClassifier_4 subset 25\n",
      "  1st level model_4 score: 0.64533\n",
      "\n",
      "  1st level model_4 gini score: 0.29065\n",
      "\n",
      "\n",
      "\n",
      "Correlation between without-one-column predictions from 1st level models:\n",
      "\n",
      "\n",
      "                   1st level model_1  1st level model_2  1st level model_3  \\\n",
      "1st level model_1           1.000000           0.982256           0.982814   \n",
      "1st level model_2           0.982256           1.000000           0.999909   \n",
      "1st level model_3           0.982814           0.999909           1.000000   \n",
      "1st level model_4           0.988855           0.994935           0.995452   \n",
      "\n",
      "                   1st level model_4  \n",
      "1st level model_1           0.988855  \n",
      "1st level model_2           0.994935  \n",
      "1st level model_3           0.995452  \n",
      "1st level model_4           1.000000  \n",
      "\n",
      "\n",
      "Fit LogisticRegression_1\n",
      "  2nd level model_1 score: 0.64605\n",
      "\n",
      "Fit ExtraTreesClassifier_2\n",
      "  2nd level model_2 score: 0.64648\n",
      "\n",
      "\n",
      "\n",
      "Correlation between final predictions from 2nd level models:\n",
      "\n",
      "\n",
      "                   2nd level model_1  2nd level model_2\n",
      "2nd level model_1           1.000000           0.901101\n",
      "2nd level model_2           0.901101           1.000000\n",
      "\n",
      "\n",
      "2nd level models final score: 0.64640\n",
      "2nd level models final gini score: 0.29279\n"
     ]
    }
   ],
   "source": [
    "# Mode 2 run\n",
    "stack = Ensemble(mode=2,\n",
    "        n_splits=5,\n",
    "        stacker_2 = (log_model, et_model),         \n",
    "        stacker_1 = (log_model, et_model, et_model2, mlp_model),\n",
    "        base_models = (\n",
    "            lgb_model_1, lgb_model_2, lgb_model_3, lgb_model_4, lgb_model_5, lgb_model_6,\n",
    "            lgb_model_7, lgb_model_8, lgb_model_9, lgb_model_10, \n",
    "            xgb_model_1, xgb_model_2, xgb_model_3, xgb_model_4, xgb_model_5, \n",
    "            xgb_model_6, xgb_model_7, xgb_model_8, xgb_model_9, xgb_model_10,\n",
    "            xgb_model_11, xgb_model_12, xgb_model_13, xgb_model_14, xgb_model_15\n",
    "            \n",
    "        ))       \n",
    "        \n",
    "y_pred = stack.fit_predict(train, target_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "# fn = '../submissions/sub.xgb.{}.{}GMT'.format(full_oof_score, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_score = 0.29279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submission from mode 2\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_pred\n",
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "fn = '../submissions/sub.2l.{}.{}GMT'.format(final_score, now)\n",
    "sub.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub.2l.0.29279.2017_11_29_20_47_18GMT\n"
     ]
    }
   ],
   "source": [
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
